{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f551ad6",
   "metadata": {},
   "source": [
    "## Multi-Modal RAG with Azure AI Content Understanding - Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84de690c",
   "metadata": {},
   "source": [
    "![rag_data_prep](./Assets/rag_data_prep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b35250",
   "metadata": {},
   "source": [
    "### Install Dependencies and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e36949f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.0.1)\n",
      "Collecting pdfminer.six\n",
      "  Downloading pdfminer_six-20260107-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: openai in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (2.15.0)\n",
      "Requirement already satisfied: azure-identity in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.20.0)\n",
      "Requirement already satisfied: azure-storage-blob in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (12.28.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pdfminer.six) (3.4.1)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pdfminer.six) (44.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from openai) (0.12.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from openai) (2.12.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: azure-core>=1.31.0 in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from azure-identity) (1.32.0)\n",
      "Requirement already satisfied: msal>=1.30.0 in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from azure-identity) (1.31.1)\n",
      "Requirement already satisfied: msal-extensions>=1.2.0 in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from azure-identity) (1.2.0)\n",
      "Requirement already satisfied: isodate>=0.6.1 in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from azure-storage-blob) (0.7.2)\n",
      "Requirement already satisfied: requests>=2.21.0 in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from azure-core>=1.31.0->azure-identity) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from azure-core>=1.31.0->azure-identity) (1.17.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity) (2.10.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (2.3.0)\n",
      "Requirement already satisfied: portalocker<3,>=1.4 in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from msal-extensions>=1.2.0->azure-identity) (2.10.1)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from portalocker<3,>=1.4->msal-extensions>=1.2.0->azure-identity) (308)\n",
      "Requirement already satisfied: colorama in c:\\users\\asridharan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading pdfminer_six-20260107-py3-none-any.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   ---------------------------------------  6.6/6.6 MB 36.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 31.2 MB/s  0:00:00\n",
      "Installing collected packages: pdfminer.six\n",
      "Successfully installed pdfminer.six-20260107\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\asridharan\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install python-dotenv pdfminer.six openai azure-identity azure-storage-blob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e29d30",
   "metadata": {},
   "source": [
    "### Setting Up the Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2277abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using Entra ID authentication\n",
      "Endpoint: https://demo-aisvcs-eus.services.ai.azure.com\n",
      "Storage: https://multimodalragsa2.blob.core.windows.net/ragdata\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import json\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Content Understanding Service Connections\n",
    "CONTENT_UNDERSTANDING_ENDPOINT = os.getenv(\"CONTENT_UNDERSTANDING_ENDPOINT\").strip().rstrip('/')\n",
    "CONTENT_UNDERSTANDING_API_KEY = os.getenv(\"CONTENT_UNDERSTANDING_API_KEY\", \"\").strip()\n",
    "\n",
    "# Storage Account Connections\n",
    "storage_account_endpoint = os.getenv(\"STORAGE_ACCOUNT_ENDPOINT\").strip().rstrip('/')\n",
    "storage_container_name = os.getenv(\"STORAGE_CONTAINER_NAME\")\n",
    "\n",
    "# Authentication Mode\n",
    "AUTH_MODE = os.getenv(\"AUTH_MODE\", \"entra\").lower()  # \"entra\" or \"key\"\n",
    "\n",
    "# API Version - using GA API\n",
    "API_VERSION = \"2025-11-01\"\n",
    "\n",
    "# Setup credential for Entra ID auth\n",
    "credential = None\n",
    "if AUTH_MODE == \"entra\":\n",
    "    credential = DefaultAzureCredential()\n",
    "    print(\"âœ… Using Entra ID authentication\")\n",
    "else:\n",
    "    print(\"ðŸ”‘ Using API Key authentication\")\n",
    "\n",
    "def get_headers():\n",
    "    \"\"\"Get appropriate headers based on auth mode.\"\"\"\n",
    "    if AUTH_MODE == \"entra\" and credential:\n",
    "        token = credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "        return {\n",
    "            \"Authorization\": f\"Bearer {token.token}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Ocp-Apim-Subscription-Key\": CONTENT_UNDERSTANDING_API_KEY\n",
    "        }\n",
    "\n",
    "print(f\"Endpoint: {CONTENT_UNDERSTANDING_ENDPOINT}\")\n",
    "print(f\"Storage: {storage_account_endpoint}/{storage_container_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a7e30b",
   "metadata": {},
   "source": [
    "### Creating Functions for Performing Video Analysis using Prebuilt Video Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "858b438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to analyze video using prebuilt video analyzer - makes API calls to Content Understanding service\n",
    "# GA API uses \"prebuilt-video\" (not \"prebuilt-videoAnalyzer\")\n",
    "def analyze_video(video_url):\n",
    "    prebuilt_video_analyzer_url = f\"{CONTENT_UNDERSTANDING_ENDPOINT}/contentunderstanding/analyzers/prebuilt-video:analyze?api-version={API_VERSION}\"\n",
    "\n",
    "    # GA API uses \"inputs\" array format\n",
    "    body = {\n",
    "        \"inputs\": [{\"url\": video_url}]\n",
    "    }\n",
    "\n",
    "    video_analysis_result = {}\n",
    "\n",
    "    try:\n",
    "        headers = get_headers()\n",
    "\n",
    "        response = requests.post(prebuilt_video_analyzer_url, headers=headers, json=body)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # GA API returns operation-location in header\n",
    "        operation_location = response.headers.get(\"Operation-Location\")\n",
    "        if operation_location:\n",
    "            get_result_url = operation_location\n",
    "        else:\n",
    "            result = response.json()\n",
    "            analysis_id = result.get(\"id\")\n",
    "            print(\"Analysis ID:\", analysis_id)\n",
    "            get_result_url = f\"{CONTENT_UNDERSTANDING_ENDPOINT}/contentunderstanding/analyzerResults/{analysis_id}?api-version={API_VERSION}\"\n",
    "        \n",
    "        print(f\"Operation URL: {get_result_url}\")\n",
    "\n",
    "        # Poll for results\n",
    "        analysis_status = \"Running\"\n",
    "        while analysis_status in [\"Running\", \"NotStarted\"]:\n",
    "            import time\n",
    "            time.sleep(10)  # Wait before polling\n",
    "            headers_poll = get_headers()\n",
    "            if \"Content-Type\" in headers_poll:\n",
    "                del headers_poll[\"Content-Type\"]\n",
    "            status_response = requests.get(get_result_url, headers=headers_poll)\n",
    "            status_response.raise_for_status()\n",
    "            status_result = status_response.json()\n",
    "            analysis_status = status_result.get(\"status\")\n",
    "            print(\"Current Analysis Status:\", analysis_status)\n",
    "            \n",
    "        video_analysis_result = status_result\n",
    "        print(\"Video Analysis Complete!\")\n",
    "        return video_analysis_result\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        if hasattr(e, 'response') and e.response is not None:\n",
    "            print(f\"Response: {e.response.text}\")\n",
    "\n",
    "\n",
    "# Creating a Function to extract relevant information from the video analysis result and prepare it for RAG\n",
    "def build_simple_video_dataset(\n",
    "    analysis_result,\n",
    "    video_url,\n",
    "    title,\n",
    "    include_full_transcript_row=True,\n",
    "    include_words=False,          # set True to include word-level timings in each phrase\n",
    "    ensure_ascii=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds dataset rows with segments + their transcripts.\n",
    "    Output rows: [{\"document_title\": ..., \"content_text\": <JSON string>}]\n",
    "\n",
    "    content_text JSON schema (per segment):\n",
    "    {\n",
    "      \"about\": \"...\",\n",
    "      \"video_url\": \"...\",\n",
    "      \"segment_id\": \"...\",\n",
    "      \"time_window\": {\"start_ms\": int, \"end_ms\": int},\n",
    "      \"segment_description\": \"...\",\n",
    "      \"transcript\": [\n",
    "        {\n",
    "          \"speaker\": \"Speaker 1\",\n",
    "          \"start_ms\": 1360,\n",
    "          \"end_ms\": 6640,\n",
    "          \"text\": \"...\",\n",
    "          \"confidence\": 0.937,\n",
    "          \"words\": [ { \"start_ms\": ..., \"end_ms\": ..., \"text\": \"...\" }, ... ]   # only if include_words=True\n",
    "        },\n",
    "        ...\n",
    "      ],\n",
    "      \"transcript_text\": \"Concatenated transcript text for this segment\"\n",
    "    }\n",
    "\n",
    "    If include_full_transcript_row=True, an extra row is appended with the entire transcript.\n",
    "    \"\"\"\n",
    "    result = (analysis_result or {}).get(\"result\", {}) or {}\n",
    "    contents = result.get(\"contents\") or []\n",
    "\n",
    "    # --- Collect segments from both shapes ---\n",
    "    segments = []\n",
    "    for c in contents:\n",
    "        # Shape 1\n",
    "        for s in c.get(\"segments\") or []:\n",
    "            segments.append({\n",
    "                \"segmentId\": s.get(\"segmentId\"),\n",
    "                \"startTimeMs\": s.get(\"startTimeMs\"),\n",
    "                \"endTimeMs\": s.get(\"endTimeMs\"),\n",
    "                \"description\": s.get(\"description\") or \"\"\n",
    "            })\n",
    "        # Shape 2\n",
    "        fields = c.get(\"fields\") or {}\n",
    "        f_segments = ((fields.get(\"Segments\") or {}).get(\"valueArray\")) or []\n",
    "        for item in f_segments:\n",
    "            vo = (item or {}).get(\"valueObject\") or {}\n",
    "            segments.append({\n",
    "                \"segmentId\": ((vo.get(\"SegmentId\") or {}).get(\"valueString\")),\n",
    "                \"startTimeMs\": ((vo.get(\"StartTimeMs\") or {}).get(\"valueInteger\")),\n",
    "                \"endTimeMs\": ((vo.get(\"EndTimeMs\") or {}).get(\"valueInteger\")),\n",
    "                \"description\": ((vo.get(\"Description\") or {}).get(\"valueString\")) or \"\"\n",
    "            })\n",
    "\n",
    "    # --- Collect transcript phrases ---\n",
    "    all_phrases = []\n",
    "    for c in contents:\n",
    "        for p in c.get(\"transcriptPhrases\") or []:\n",
    "            phrase = {\n",
    "                \"speaker\": p.get(\"speaker\"),\n",
    "                \"start_ms\": p.get(\"startTimeMs\"),\n",
    "                \"end_ms\": p.get(\"endTimeMs\"),\n",
    "                \"text\": p.get(\"text\"),\n",
    "                \"confidence\": p.get(\"confidence\")\n",
    "            }\n",
    "            if include_words:\n",
    "                phrase[\"words\"] = [\n",
    "                    {\n",
    "                        \"start_ms\": w.get(\"startTimeMs\"),\n",
    "                        \"end_ms\": w.get(\"endTimeMs\"),\n",
    "                        \"text\": w.get(\"text\")\n",
    "                    } for w in (p.get(\"words\") or [])\n",
    "                ]\n",
    "            all_phrases.append(phrase)\n",
    "\n",
    "    def overlaps(seg_start, seg_end, p_start, p_end):\n",
    "        \"\"\"True if [p_start, p_end] overlaps [seg_start, seg_end].\"\"\"\n",
    "        if seg_start is None or seg_end is None or p_start is None or p_end is None:\n",
    "            return False\n",
    "        return not (p_end < seg_start or p_start > seg_end)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    # If no segments, create a single fallback with full transcript (if present)\n",
    "    if not segments:\n",
    "        # Build overall transcript text (if any)\n",
    "        overall_text = \" \".join([p.get(\"text\") or \"\" for p in all_phrases]).strip() if all_phrases else None\n",
    "\n",
    "        content_text_obj = {\n",
    "            \"about\": \"This is a JSON string representing the overall video summary.\",\n",
    "            \"video_url\": video_url,\n",
    "            \"note\": \"No segment details available.\"\n",
    "        }\n",
    "        if overall_text:\n",
    "            content_text_obj[\"full_transcript_text\"] = overall_text\n",
    "            content_text_obj[\"full_transcript\"] = all_phrases  # could be large\n",
    "\n",
    "        rows.append({\n",
    "            \"document_title\": f\"{title} â€¢ Full Video\",\n",
    "            \"content_text\": json.dumps(content_text_obj, ensure_ascii=ensure_ascii)\n",
    "        })\n",
    "        return rows\n",
    "\n",
    "    # Build rows per segment with attached transcript snippets\n",
    "    for s in segments:\n",
    "        st, et = s.get(\"startTimeMs\"), s.get(\"endTimeMs\")\n",
    "        seg_id = s.get(\"segmentId\") or \"?\"\n",
    "        seg_desc = s.get(\"description\") or \"\"\n",
    "\n",
    "        # collect overlapping phrases\n",
    "        seg_phrases = [\n",
    "            p for p in all_phrases\n",
    "            if overlaps(st, et, p.get(\"start_ms\"), p.get(\"end_ms\"))\n",
    "        ]\n",
    "\n",
    "        # Concatenate a readable segment transcript\n",
    "        seg_transcript_text = \" \".join([(p.get(\"text\") or \"\").strip() for p in seg_phrases]).strip()\n",
    "\n",
    "        content_text_obj = {\n",
    "            \"about\": \"This is a JSON string representing a slice of video analysis for RAG.\",\n",
    "            \"video_url\": video_url,\n",
    "            \"segment_id\": seg_id,\n",
    "            \"time_window\": {\"start_ms\": st, \"end_ms\": et},\n",
    "            \"segment_description\": seg_desc,\n",
    "            \"transcript\": seg_phrases,\n",
    "            \"transcript_text\": seg_transcript_text\n",
    "        }\n",
    "\n",
    "        rows.append({\n",
    "            \"document_title\": f\"{title} â€¢ Segment {seg_id}\",\n",
    "            \"content_text\": json.dumps(content_text_obj, ensure_ascii=ensure_ascii)\n",
    "        })\n",
    "\n",
    "    # Optional final row with the full transcript (nice for global search)\n",
    "    if include_full_transcript_row and all_phrases:\n",
    "        full_text = \" \".join([(p.get(\"text\") or \"\").strip() for p in all_phrases]).strip()\n",
    "        full_obj = {\n",
    "            \"about\": \"This is a JSON string representing the overall video transcript.\",\n",
    "            \"video_url\": video_url,\n",
    "            \"transcript\": all_phrases,\n",
    "            \"transcript_text\": full_text\n",
    "        }\n",
    "        rows.append({\n",
    "            \"document_title\": f\"{title} â€¢ Full Transcript\",\n",
    "            \"content_text\": json.dumps(full_obj, ensure_ascii=ensure_ascii)\n",
    "        })\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13c63e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation URL: https://demo-aisvcs-eus.services.ai.azure.com/contentunderstanding/analyzerResults/ced64248-aebe-4138-b290-23223a24bf34?api-version=2025-11-01\n",
      "Current Analysis Status: Running\n",
      "Current Analysis Status: Succeeded\n",
      "Video Analysis Complete!\n",
      "[\n",
      "  {\n",
      "    \"document_title\": \"BMW_circularity_video \\u2022 Full Video\",\n",
      "    \"content_text\": \"{\\\"about\\\": \\\"This is a JSON string representing the overall video summary.\\\", \\\"video_url\\\": \\\"https://multimodalragsa2.blob.core.windows.net/ragdata/BMW_circularity.mp4\\\", \\\"note\\\": \\\"No segment details available.\\\"}\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Constructing the Video URL\n",
    "video_url = f\"{storage_account_endpoint}/{storage_container_name}/BMW_circularity.mp4\"\n",
    "\n",
    "# Analyzing the video and building the dataset\n",
    "video_analysis_result = analyze_video(video_url)\n",
    "video_dataset = build_simple_video_dataset(video_analysis_result, video_url, title=\"BMW_circularity_video\")\n",
    "\n",
    "# print the dataset in a pretty format\n",
    "print(json.dumps(video_dataset, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218878dc",
   "metadata": {},
   "source": [
    "### Creating Functions for Performing Audio Analysis using Prebuilt Audio Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89451888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to analyze audio using prebuilt audio analyzer - makes API calls to Content Understanding service\n",
    "# GA API uses \"prebuilt-audio\" (not \"prebuilt-audioAnalyzer\")\n",
    "def analyze_audio(audio_url):\n",
    "    prebuilt_audio_analyzer_url = f\"{CONTENT_UNDERSTANDING_ENDPOINT}/contentunderstanding/analyzers/prebuilt-audio:analyze?api-version={API_VERSION}\"\n",
    "\n",
    "    # GA API uses \"inputs\" array format\n",
    "    body = {\n",
    "        \"inputs\": [{\"url\": audio_url}]\n",
    "    }\n",
    "\n",
    "    audio_analysis_result = {}\n",
    "\n",
    "    try:\n",
    "        headers = get_headers()\n",
    "\n",
    "        response = requests.post(prebuilt_audio_analyzer_url, headers=headers, json=body)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # GA API returns operation-location in header\n",
    "        operation_location = response.headers.get(\"Operation-Location\")\n",
    "        if operation_location:\n",
    "            get_result_url = operation_location\n",
    "        else:\n",
    "            result = response.json()\n",
    "            analysis_id = result.get(\"id\")\n",
    "            print(\"Analysis ID:\", analysis_id)\n",
    "            get_result_url = f\"{CONTENT_UNDERSTANDING_ENDPOINT}/contentunderstanding/analyzerResults/{analysis_id}?api-version={API_VERSION}\"\n",
    "        \n",
    "        print(f\"Operation URL: {get_result_url}\")\n",
    "\n",
    "        # Poll for results\n",
    "        analysis_status = \"Running\"\n",
    "        while analysis_status in [\"Running\", \"NotStarted\"]:\n",
    "            import time\n",
    "            time.sleep(10)  # Wait before polling\n",
    "            headers_poll = get_headers()\n",
    "            if \"Content-Type\" in headers_poll:\n",
    "                del headers_poll[\"Content-Type\"]\n",
    "            status_response = requests.get(get_result_url, headers=headers_poll)\n",
    "            status_response.raise_for_status()\n",
    "            status_result = status_response.json()\n",
    "            analysis_status = status_result.get(\"status\")\n",
    "            print(\"Current Analysis Status:\", analysis_status)\n",
    "            \n",
    "        audio_analysis_result = status_result\n",
    "        print(\"Audio Analysis Complete!\")\n",
    "        return audio_analysis_result\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        if hasattr(e, 'response') and e.response is not None:\n",
    "            print(f\"Response: {e.response.text}\")\n",
    "\n",
    "# Creating a Function to extract relevant information from the audio analysis result and prepare it for RAG\n",
    "def build_simple_audio_dataset(analysis_result, audio_url, title, include_full_transcript_row=True):\n",
    "    \"\"\"\n",
    "    Create simple dataset rows with document_title + content_text from audio analysis.\n",
    "    \"\"\"\n",
    "    result = (analysis_result or {}).get(\"result\", {}) or {}\n",
    "    contents = result.get(\"contents\") or []\n",
    "\n",
    "    rows = []\n",
    "    all_phrases = []\n",
    "\n",
    "    for idx, item in enumerate(contents, start=1):\n",
    "        kind = item.get(\"kind\")\n",
    "        st, et = item.get(\"startTimeMs\"), item.get(\"endTimeMs\")\n",
    "        md = item.get(\"markdown\") or \"\"\n",
    "        phrases = item.get(\"transcriptPhrases\") or []\n",
    "        all_phrases.extend(phrases)\n",
    "\n",
    "        # Optional summary field (if present)\n",
    "        fields = item.get(\"fields\") or {}\n",
    "        summary = ((fields.get(\"Summary\") or {}).get(\"valueString\")) if fields else None\n",
    "\n",
    "        # Short transcript excerpt\n",
    "        transcript_excerpt = \"\"\n",
    "        if phrases:\n",
    "            transcript_excerpt = \" \".join((p.get(\"text\") or \"\").strip() for p in phrases[:3]).strip()\n",
    "\n",
    "        content_text = json.dumps({\n",
    "            \"about\": \"This is a JSON string representing a slice of audio analysis for RAG.\",\n",
    "            \"audio_url\": audio_url,\n",
    "            \"content_index\": idx,\n",
    "            \"kind\": kind,\n",
    "            \"time_window\": {\"start_ms\": st, \"end_ms\": et},\n",
    "            \"analyzer_markdown_excerpt\": md[:400],\n",
    "            \"transcript_excerpt\": transcript_excerpt,\n",
    "            **({\"summary\": summary} if summary else {})\n",
    "        }, ensure_ascii=False)\n",
    "\n",
    "        rows.append({\n",
    "            \"document_title\": f\"{title} â€¢ Segment {idx}\",\n",
    "            \"content_text\": content_text\n",
    "        })\n",
    "\n",
    "    # Optionally add a full transcript row for global search\n",
    "    if include_full_transcript_row and all_phrases:\n",
    "        full_transcript_text = \" \".join((p.get(\"text\") or \"\").strip() for p in all_phrases).strip()\n",
    "        full_obj = {\n",
    "            \"about\": \"This is a JSON string representing the full audio transcript.\",\n",
    "            \"audio_url\": audio_url,\n",
    "            \"transcript_text\": full_transcript_text,\n",
    "            \"transcript\": [\n",
    "                {\n",
    "                    \"speaker\": p.get(\"speaker\"),\n",
    "                    \"start_ms\": p.get(\"startTimeMs\"),\n",
    "                    \"end_ms\": p.get(\"endTimeMs\"),\n",
    "                    \"text\": p.get(\"text\"),\n",
    "                    \"confidence\": p.get(\"confidence\")\n",
    "                } for p in all_phrases\n",
    "            ]\n",
    "        }\n",
    "        rows.append({\n",
    "            \"document_title\": f\"{title} â€¢ Full Transcript\",\n",
    "            \"content_text\": json.dumps(full_obj, ensure_ascii=False)\n",
    "        })\n",
    "\n",
    "    # Fallback if nothing produced\n",
    "    if not rows:\n",
    "        content_text = json.dumps({\n",
    "            \"about\": \"This is a JSON string representing a generic audio analysis record.\",\n",
    "            \"audio_url\": audio_url,\n",
    "            \"note\": \"No content segments found in analysis.\"\n",
    "        }, ensure_ascii=False)\n",
    "        rows.append({\n",
    "            \"document_title\": f\"{title} â€¢ Full Audio\",\n",
    "            \"content_text\": content_text\n",
    "        })\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1a1a2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation URL: https://demo-aisvcs-eus.services.ai.azure.com/contentunderstanding/analyzerResults/9e6466a4-ab7c-4298-9ff2-b8423c2c0dec?api-version=2025-11-01\n",
      "Current Analysis Status: Running\n",
      "Current Analysis Status: Running\n",
      "Current Analysis Status: Running\n",
      "Current Analysis Status: Succeeded\n",
      "Audio Analysis Complete!\n",
      "[\n",
      "  {\n",
      "    \"document_title\": \"BMW_forwardism_audio \\u2022 Segment 1\",\n",
      "    \"content_text\": \"{\\\"about\\\": \\\"This is a JSON string representing a slice of audio analysis for RAG.\\\", \\\"audio_url\\\": \\\"https://multimodalragsa2.blob.core.windows.net/ragdata/BMW_forwardism.mp3\\\", \\\"content_index\\\": 1, \\\"kind\\\": \\\"audioVisual\\\", \\\"time_window\\\": {\\\"start_ms\\\": 0, \\\"end_ms\\\": 405513}, \\\"analyzer_markdown_excerpt\\\": \\\"# Audio: 00:00.000 => 06:45.513\\\\n\\\\nTranscript\\\\n```\\\\nWEBVTT\\\\n\\\\n00:02.480 --> 00:03.040\\\\n<v Speaker 1>Wow.\\\\n\\\\n00:04.000 --> 00:05.680\\\\n<v Speaker 1>Time flies when you're having fun.\\\\n\\\\n00:06.560 --> 00:16.960\\\\n<v Speaker 1>It was such an honor to be able to talk to so many inspiring guests, artists, chefs, designers, and many more who are stepping up and moving our world forward.\\\\n\\\\n00:18.320 --> 00:21.440\\\\n<v Spe\\\", \\\"transcript_excerpt\\\": \\\"\\\"}\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Constructing the Audio URL\n",
    "audio_url = f\"{storage_account_endpoint}/{storage_container_name}/BMW_forwardism.mp3\"\n",
    "\n",
    "# Analyzing the audio and building the dataset\n",
    "audio_analysis_result = analyze_audio(audio_url)\n",
    "audio_dataset = build_simple_audio_dataset(audio_analysis_result, audio_url, title=\"BMW_forwardism_audio\")\n",
    "\n",
    "# print the dataset in a pretty format\n",
    "print(json.dumps(audio_dataset, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b6b657",
   "metadata": {},
   "source": [
    "### Creating Functions for Performing Image Analysis using Prebuilt Image Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7674e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to analyze image using prebuilt image analyzer - makes API calls to Content Understanding service\n",
    "# GA API uses \"prebuilt-image\" (not \"prebuilt-imageAnalyzer\")\n",
    "def analyze_image(image_url):\n",
    "    prebuilt_image_analyzer_url = f\"{CONTENT_UNDERSTANDING_ENDPOINT}/contentunderstanding/analyzers/prebuilt-image:analyze?api-version={API_VERSION}\"\n",
    "\n",
    "    body = {\n",
    "        \"inputs\": [{\"url\": image_url}]\n",
    "    }\n",
    "\n",
    "    image_analysis_result = {}\n",
    "\n",
    "    try:\n",
    "        headers = get_headers()\n",
    "\n",
    "        response = requests.post(prebuilt_image_analyzer_url, headers=headers, json=body)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # For GA API, check if we get operation-location header for async processing\n",
    "        if response.status_code == 202:\n",
    "            # Async operation - get the result URL from header\n",
    "            operation_location = response.headers.get(\"operation-location\")\n",
    "            if operation_location:\n",
    "                get_result_url = operation_location\n",
    "            else:\n",
    "                result = response.json()\n",
    "                analysis_id = result.get(\"id\")\n",
    "                print(\"Analysis ID:\", analysis_id)\n",
    "                get_result_url = f\"{CONTENT_UNDERSTANDING_ENDPOINT}/contentunderstanding/analyzerResults/{analysis_id}?api-version={API_VERSION}\"\n",
    "        else:\n",
    "            result = response.json()\n",
    "            analysis_id = result.get(\"id\")\n",
    "            print(\"Analysis ID:\", analysis_id)\n",
    "            get_result_url = f\"{CONTENT_UNDERSTANDING_ENDPOINT}/contentunderstanding/analyzerResults/{analysis_id}?api-version={API_VERSION}\"\n",
    "\n",
    "        # Using the result URL to get results; polling until the analysis is complete\n",
    "        poll_headers = get_headers()\n",
    "        analysis_status = \"Running\"\n",
    "        while analysis_status == \"Running\":\n",
    "            status_response = requests.get(get_result_url, headers=poll_headers)\n",
    "            status_response.raise_for_status()\n",
    "            status_result = status_response.json()\n",
    "            analysis_status = status_result.get(\"status\")\n",
    "            print(\"Current Analysis Status:\", analysis_status)\n",
    "            if analysis_status == \"Running\":\n",
    "                import time\n",
    "                time.sleep(1)  # Wait before polling again\n",
    "        result_response = requests.get(get_result_url, headers=poll_headers)\n",
    "        result_response.raise_for_status()\n",
    "        image_analysis_result = result_response.json()\n",
    "        print(\"Image Analysis Result:\", image_analysis_result)\n",
    "        return image_analysis_result\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "\n",
    "# Creating a Function to extract relevant information from the image analysis result and prepare it for RAG\n",
    "def build_simple_image_dataset(analysis_result, image_url, title):\n",
    "    \"\"\"\n",
    "    Create simple dataset rows with document_title + content_text from image analysis.\n",
    "    Pulls 'Summary' from fields if present.\n",
    "    \"\"\"\n",
    "    result = (analysis_result or {}).get(\"result\", {}) or {}\n",
    "    contents = result.get(\"contents\") or []\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for idx, item in enumerate(contents, start=1):\n",
    "        kind = item.get(\"kind\")\n",
    "        md = item.get(\"markdown\") or \"\"\n",
    "        fields = item.get(\"fields\") or {}\n",
    "\n",
    "        # Extract a clean summary string if present\n",
    "        summary_obj = fields.get(\"Summary\") or {}\n",
    "        summary_text = summary_obj.get(\"valueString\")\n",
    "\n",
    "        payload = {\n",
    "            \"about\": \"This is a JSON string representing a slice of image analysis for RAG.\",\n",
    "            \"image_url\": image_url,\n",
    "            \"content_index\": idx,\n",
    "            \"kind\": kind,\n",
    "            \"analyzer_markdown_excerpt\": md[:400],\n",
    "        }\n",
    "        if summary_text:\n",
    "            payload[\"summary\"] = summary_text\n",
    "        else:\n",
    "            # If you prefer preserving all fields, keep themâ€”but they're verbose\n",
    "            payload[\"fields_raw\"] = fields\n",
    "\n",
    "        rows.append({\n",
    "            \"document_title\": f\"{title} â€¢ Content {idx}\",\n",
    "            \"content_text\": json.dumps(payload, ensure_ascii=False)\n",
    "        })\n",
    "\n",
    "    if not rows:\n",
    "        rows.append({\n",
    "            \"document_title\": f\"{title} â€¢ Full Image\",\n",
    "            \"content_text\": json.dumps({\n",
    "                \"about\": \"This is a JSON string representing a generic image analysis record.\",\n",
    "                \"image_url\": image_url,\n",
    "                \"note\": \"No content found in analysis.\"\n",
    "            }, ensure_ascii=False)\n",
    "        })\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d7360fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred: 400 Client Error: Bad Request for url: https://demo-aisvcs-eus.services.ai.azure.com/contentunderstanding/analyzers/prebuilt-image:analyze?api-version=2025-11-01\n",
      "[\n",
      "  {\n",
      "    \"document_title\": \"Sample Image \\u2022 Full Image\",\n",
      "    \"content_text\": \"{\\\"about\\\": \\\"This is a JSON string representing a generic image analysis record.\\\", \\\"image_url\\\": \\\"https://multimodalragsa2.blob.core.windows.net/ragdata/image.png\\\", \\\"note\\\": \\\"No content found in analysis.\\\"}\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Constructing the Image URL\n",
    "image_url = f\"{storage_account_endpoint}/{storage_container_name}/image.png\"\n",
    "\n",
    "# Analyzing the image and building the dataset\n",
    "image_analysis_result = analyze_image(image_url)\n",
    "image_dataset = build_simple_image_dataset(image_analysis_result, image_url, title=\"Sample Image\")\n",
    "\n",
    "# print the dataset in a pretty format\n",
    "print(json.dumps(image_dataset, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53a813f",
   "metadata": {},
   "source": [
    "### Extracting Textual Data from PDF Document and Preparing it for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e232750a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "from pdfminer.high_level import extract_text_to_fp\n",
    "from pdfminer.layout import LAParams\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "def _is_url(s: str) -> bool:\n",
    "    return s.lower().startswith((\"http://\", \"https://\"))\n",
    "\n",
    "def _download_bytes_with_entra(url: str, credential) -> bytes:\n",
    "    \"\"\"Download blob bytes using Entra ID authentication.\"\"\"\n",
    "    # Parse the blob URL to extract account, container, and blob name\n",
    "    parsed = urlparse(url)\n",
    "    account_url = f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "    path_parts = parsed.path.lstrip('/').split('/', 1)\n",
    "    container_name = path_parts[0]\n",
    "    blob_name = path_parts[1] if len(path_parts) > 1 else \"\"\n",
    "    \n",
    "    # Use BlobServiceClient with Entra ID credential\n",
    "    blob_service_client = BlobServiceClient(account_url=account_url, credential=credential)\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "    \n",
    "    return blob_client.download_blob().readall()\n",
    "\n",
    "def _clean_text(text: str) -> str:\n",
    "    text = text.replace(\"\\r\", \"\")\n",
    "    text = re.sub(r\"[ \\t]+\\n\", \"\\n\", text)\n",
    "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def extract_pdf_text(pdf_source: str, credential=None):\n",
    "    \"\"\"\n",
    "    Extracts plain text from each page of a PDF.\n",
    "    pdf_source can be a local file path or a URL.\n",
    "    credential: Azure credential for accessing private blob storage (optional)\n",
    "    Returns: list of {\"pageNumber\": int, \"text\": str}\n",
    "    \"\"\"\n",
    "    if _is_url(pdf_source):\n",
    "        if credential:\n",
    "            data = _download_bytes_with_entra(pdf_source, credential)\n",
    "        else:\n",
    "            # Fallback to anonymous download for public blobs\n",
    "            r = requests.get(pdf_source, timeout=30)\n",
    "            r.raise_for_status()\n",
    "            data = r.content\n",
    "        fp = io.BytesIO(data)\n",
    "    else:\n",
    "        fp = open(pdf_source, \"rb\")\n",
    "\n",
    "    pages = []\n",
    "    try:\n",
    "        output = io.StringIO()\n",
    "        laparams = LAParams()\n",
    "        extract_text_to_fp(fp, output, laparams=laparams, output_type=\"text\")\n",
    "        full_text = output.getvalue()\n",
    "        raw_pages = full_text.split(\"\\x0c\")  # pdfminer page delimiter\n",
    "        for i, txt in enumerate(raw_pages, start=1):\n",
    "            cleaned = _clean_text(txt)\n",
    "            if i == len(raw_pages) and not cleaned:\n",
    "                continue\n",
    "            pages.append({\"pageNumber\": i, \"text\": cleaned})\n",
    "    finally:\n",
    "        fp.close()\n",
    "    return pages\n",
    "\n",
    "\n",
    "def build_simple_pdf_dataset(pages, pdf_source: str, title: str):\n",
    "    \"\"\"\n",
    "    Convert PDF pages into dataset rows like your audio/video/image functions.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for page in pages:\n",
    "        pno = page[\"pageNumber\"]\n",
    "        text = page[\"text\"]\n",
    "\n",
    "        payload = {\n",
    "            \"about\": \"This is a JSON string representing a slice of PDF for RAG.\",\n",
    "            \"pdf_source\": pdf_source,\n",
    "            \"page_number\": pno,\n",
    "            \"page_text\": text\n",
    "        }\n",
    "\n",
    "        rows.append({\n",
    "            \"document_title\": f\"{title} â€¢ Page {pno}\",\n",
    "            \"content_text\": json.dumps(payload, ensure_ascii=False)\n",
    "        })\n",
    "\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbe102ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"document_title\": \"BMW Sustainable Natural Rubber PDF \\u2022 Page 1\",\n",
      "    \"content_text\": \"{\\\"about\\\": \\\"This is a JSON string representing a slice of PDF for RAG.\\\", \\\"pdf_source\\\": \\\"https://multimodalragsa2.blob.core.windows.net/ragdata/BMW_sustainable_natural_rubber.pdf\\\", \\\"page_number\\\": 1, \\\"page_text\\\": \\\"High-Level Commitment of the BMW Group\\\\nfor Sustainable Natural Rubber\\\\n\\\\nProtection of forests and other natural ecosystems is critical for maintaining biodiversity, combating climate\\\\nchange, and sustaining livelihoods. As part of our overall sustainability goals, the BMW Group is committed to\\\\neliminating deforestation and ecosystem conversion from our supply chains and to safeguarding human rights\\\\nacross all our operations and suppliers. Given that natural rubber is a known driver of deforestation, this\\\\ndocument outlines our commitment to sourcing sustainable natural rubber and is aligned with the Policy\\\\nFramework that was adopted in a September 2020 resolution by the General Assembly of the Global Platform\\\\nfor Sustainable Natural Rubber (GPSNR), of which we are a founding member.\\\\n\\\\nThis document is also aligned with the principles and guidelines laid out in the UN Guiding Principles for\\\\nBusiness and Human Rights and the ILO fundamental conventions.\\\\n\\\\nScope\\\\n\\\\nThe provisions in this document apply to the products containing natural rubber that are sourced by all our\\\\noperations, recognizing the need to prioritize specific products or product types for risk mitigation actions\\\\nbased on a risk assessment. Our supplier expectations are clearly detailed in the BMW Group Supplier Code\\\\nof Conduct, which serves as the basis for how we evaluate and help our suppliers maintain or progress\\\\ntowards compliance with the sustainability requirements of our company.\\\\n\\\\nCommitments\\\\n\\\\nSustainable Natural Rubber\\\\nWe will work within our own operations and through our suppliers to increasingly source products containing\\\\nnatural rubber that is produced and processed in line with GPSNR policy requirements, such that it:\\\\n\\\\no  complies with local, national and international laws\\\\no  prohibits corruption\\\\no  does not contribute to deforestation or destruction of critical wildlife habitats\\\\no  protects high conservation values (HCVs) and high carbon stock (HCS) forests (the cutoff date after\\\\n\\\\nwhich deforestation or HCV degradation is considered non-conforming with this policy is 1 April 2019).\\\\n\\\\no  protects water and soil resources\\\\no\\\\n\\\\nrespects internationally recognized human rights and upholds the UN Guiding Principles on Business\\\\nand Human Rights (UNGP)\\\\n\\\\no  ensures the ability of Indigenous Peoples and local communities (IPLCs) to give or withhold their free,\\\\nprior, and informed consent (FPIC) on any activities that might affect their rights, and respects and\\\\nrecognizes the formal and customary land rights of IPLCs in rubber producing regions, and is in\\\\naccordance with the UN Declaration on the Rights of Indigenous Peoples (UNDRIP)\\\"}\"\n",
      "  },\n",
      "  {\n",
      "    \"document_title\": \"BMW Sustainable Natural Rubber PDF \\u2022 Page 2\",\n",
      "    \"content_text\": \"{\\\"about\\\": \\\"This is a JSON string representing a slice of PDF for RAG.\\\", \\\"pdf_source\\\": \\\"https://multimodalragsa2.blob.core.windows.net/ragdata/BMW_sustainable_natural_rubber.pdf\\\", \\\"page_number\\\": 2, \\\"page_text\\\": \\\"o  complies with the ILO fundamental Conventions and all applicable local law on workers\\u2019 rights\\\\n\\\\nregarding:\\\\na)  no child labor\\\\nb)\\\\nc)\\\\nd)  no discrimination\\\\ne)  no abusive practices\\\\n\\\\nforced or compulsory labor\\\\nfreedom of association and collective bargaining\\\\n\\\\no  strives to ensure internationally recognized labor rights and complies with all applicable local laws on\\\\n\\\\nlegal working hours\\\\n\\\\nworkers\\u2019 rights including:\\\\na)\\\\nb)  safe and healthy workplaces\\\\nc)  decent living wages\\\\nd)  gender equity\\\\n\\\\no  supports the livelihoods of rubber producers, particularly small farmers, and those communities in\\\\n\\\\nrubber producing areas\\\\n\\\\nDue Diligence\\\\n\\\\nTo demonstrate progress with our commitments and recognizing the influential role our company can play in\\\\ncontributing to sector-wide transformation, the BMW Group further implements or will implement following\\\\nmeasures in line with the BMW Group Supply Chain Due Diligence:\\\\n\\\\n-  Traceability: We will promote mapping and traceability of raw materials in our supply chain to a point\\\\n\\\\nat the appropriate jurisdictional level where compliance can be verified, aligned with the\\\\nrecommendations of the forthcoming GPSNR implementation guidance.\\\\n\\\\n-  Risk assessment: We will assess actual and potential environmental and social risks within our\\\\n\\\\nnatural rubber supply chains and will prioritize risk mitigation actions as defined in the forthcoming\\\\nGPSNR implementation guidance.\\\\n\\\\n-  Supplier engagement: We will work with our existing suppliers to identify, prevent and mitigate,\\\\n\\\\nenvironmental and social harms in natural rubber supply chains that violate this high-level\\\\ncommitment, and will evaluate the environmental and social performance of new suppliers/partners\\\\nprior to engagement. We require our suppliers to set up a due diligence process aligned with GPSNR\\\\npolicy framework for producing and procuring natural rubber and to implement necessary time-bound\\\\nmeasures as defined in the forthcoming GPSNR implementation guidelines. We offer guidance and\\\\ntraining to our suppliers to support them in understanding our expectations, relying on technical\\\\nsupport from GPSNR.\\\\n\\\\n-  Dispute resolution: We have a grievance mechanism in place (available by phone under +49 89\\\\n382-71230 or by e-mail under humanrights.sscm@bmwgroup.com.) to address any complaints\\\\nregarding our or our suppliers\\u2019 operations and to resolve disputes in a fair and timely manner. We also\\\\nexpect our suppliers to have their own grievance mechanism(s).\\\\n\\\\n-  Escalation: In cases our supply chains may not conform with our Supplier Sustainability Policy, we\\\\nwill develop time-bound implementation plans for moving towards conformance and remediation of\\\\npast or ongoing harms as defined in our escalation process.\\\\n\\\\n-  Reporting: We commit to reporting on our progress in implementing these commitments as specified\\\\n\\\\nin the forthcoming GPSNR reporting requirements.\\\\n\\\\n-  Multistakeholder Engagement: We commit to engaging in and contributing to multi-stakeholder,\\\\n\\\\nlandscape and supply chain initiatives like the GPSNR as well as further interventions that enable and\\\\nenhance sustainable natural rubber uptake in the global marketplace.\\\\n\\\\nBMW-Contact: sscm@bmw.de\\\\nStatus: August 2024\\\"}\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Constructing the PDF URL\n",
    "pdf_url = f\"{storage_account_endpoint}/{storage_container_name}/BMW_sustainable_natural_rubber.pdf\"\n",
    "\n",
    "# Extracting text from the PDF and building the dataset\n",
    "# Pass credential for Entra ID authenticated blob access\n",
    "extracted_text = extract_pdf_text(pdf_url, credential=credential)\n",
    "pdf_dataset = build_simple_pdf_dataset(extracted_text, pdf_url, title=\"BMW Sustainable Natural Rubber PDF\")\n",
    "\n",
    "print(json.dumps(pdf_dataset, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415e0b9b",
   "metadata": {},
   "source": [
    "### Combining Datasets for Multi-Modal RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a613338a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset rows: 5\n",
      "[\n",
      "  {\n",
      "    \"document_title\": \"BMW_circularity_video \\u2022 Full Video\",\n",
      "    \"content_text\": \"{\\\"about\\\": \\\"This is a JSON string representing the overall video summary.\\\", \\\"video_url\\\": \\\"https://multimodalragsa2.blob.core.windows.net/ragdata/BMW_circularity.mp4\\\", \\\"note\\\": \\\"No segment details available.\\\"}\"\n",
      "  },\n",
      "  {\n",
      "    \"document_title\": \"BMW_forwardism_audio \\u2022 Segment 1\",\n",
      "    \"content_text\": \"{\\\"about\\\": \\\"This is a JSON string representing a slice of audio analysis for RAG.\\\", \\\"audio_url\\\": \\\"https://multimodalragsa2.blob.core.windows.net/ragdata/BMW_forwardism.mp3\\\", \\\"content_index\\\": 1, \\\"kind\\\": \\\"audioVisual\\\", \\\"time_window\\\": {\\\"start_ms\\\": 0, \\\"end_ms\\\": 405513}, \\\"analyzer_markdown_excerpt\\\": \\\"# Audio: 00:00.000 => 06:45.513\\\\n\\\\nTranscript\\\\n```\\\\nWEBVTT\\\\n\\\\n00:02.480 --> 00:03.040\\\\n<v Speaker 1>Wow.\\\\n\\\\n00:04.000 --> 00:05.680\\\\n<v Speaker 1>Time flies when you're having fun.\\\\n\\\\n00:06.560 --> 00:16.960\\\\n<v Speaker 1>It was such an honor to be able to talk to so many inspiring guests, artists, chefs, designers, and many more who are stepping up and moving our world forward.\\\\n\\\\n00:18.320 --> 00:21.440\\\\n<v Spe\\\", \\\"transcript_excerpt\\\": \\\"\\\"}\"\n",
      "  },\n",
      "  {\n",
      "    \"document_title\": \"Sample Image \\u2022 Full Image\",\n",
      "    \"content_text\": \"{\\\"about\\\": \\\"This is a JSON string representing a generic image analysis record.\\\", \\\"image_url\\\": \\\"https://multimodalragsa2.blob.core.windows.net/ragdata/image.png\\\", \\\"note\\\": \\\"No content found in analysis.\\\"}\"\n",
      "  },\n",
      "  {\n",
      "    \"document_title\": \"BMW Sustainable Natural Rubber PDF \\u2022 Page 1\",\n",
      "    \"content_text\": \"{\\\"about\\\": \\\"This is a JSON string representing a slice of PDF for RAG.\\\", \\\"pdf_source\\\": \\\"https://multimodalragsa2.blob.core.windows.net/ragdata/BMW_sustainable_natural_rubber.pdf\\\", \\\"page_number\\\": 1, \\\"page_text\\\": \\\"High-Level Commitment of the BMW Group\\\\nfor Sustainable Natural Rubber\\\\n\\\\nProtection of forests and other natural ecosystems is critical for maintaining biodiversity, combating climate\\\\nchange, and sustaining livelihoods. As part of our overall sustainability goals, the BMW Group is committed to\\\\neliminating deforestation and ecosystem conversion from our supply chains and to safeguarding human rights\\\\nacross all our operations and suppliers. Given that natural rubber is a known driver of deforestation, this\\\\ndocument outlines our commitment to sourcing sustainable natural rubber and is aligned with the Policy\\\\nFramework that was adopted in a September 2020 resolution by the General Assembly of the Global Platform\\\\nfor Sustainable Natural Rubber (GPSNR), of which we are a founding member.\\\\n\\\\nThis document is also aligned with the principles and guidelines laid out in the UN Guiding Principles for\\\\nBusiness and Human Rights and the ILO fundamental conventions.\\\\n\\\\nScope\\\\n\\\\nThe provisions in this document apply to the products containing natural rubber that are sourced by all our\\\\noperations, recognizing the need to prioritize specific products or product types for risk mitigation actions\\\\nbased on a risk assessment. Our supplier expectations are clearly detailed in the BMW Group Supplier Code\\\\nof Conduct, which serves as the basis for how we evaluate and help our suppliers maintain or progress\\\\ntowards compliance with the sustainability requirements of our company.\\\\n\\\\nCommitments\\\\n\\\\nSustainable Natural Rubber\\\\nWe will work within our own operations and through our suppliers to increasingly source products containing\\\\nnatural rubber that is produced and processed in line with GPSNR policy requirements, such that it:\\\\n\\\\no  complies with local, national and international laws\\\\no  prohibits corruption\\\\no  does not contribute to deforestation or destruction of critical wildlife habitats\\\\no  protects high conservation values (HCVs) and high carbon stock (HCS) forests (the cutoff date after\\\\n\\\\nwhich deforestation or HCV degradation is considered non-conforming with this policy is 1 April 2019).\\\\n\\\\no  protects water and soil resources\\\\no\\\\n\\\\nrespects internationally recognized human rights and upholds the UN Guiding Principles on Business\\\\nand Human Rights (UNGP)\\\\n\\\\no  ensures the ability of Indigenous Peoples and local communities (IPLCs) to give or withhold their free,\\\\nprior, and informed consent (FPIC) on any activities that might affect their rights, and respects and\\\\nrecognizes the formal and customary land rights of IPLCs in rubber producing regions, and is in\\\\naccordance with the UN Declaration on the Rights of Indigenous Peoples (UNDRIP)\\\"}\"\n",
      "  },\n",
      "  {\n",
      "    \"document_title\": \"BMW Sustainable Natural Rubber PDF \\u2022 Page 2\",\n",
      "    \"content_text\": \"{\\\"about\\\": \\\"This is a JSON string representing a slice of PDF for RAG.\\\", \\\"pdf_source\\\": \\\"https://multimodalragsa2.blob.core.windows.net/ragdata/BMW_sustainable_natural_rubber.pdf\\\", \\\"page_number\\\": 2, \\\"page_text\\\": \\\"o  complies with the ILO fundamental Conventions and all applicable local law on workers\\u2019 rights\\\\n\\\\nregarding:\\\\na)  no child labor\\\\nb)\\\\nc)\\\\nd)  no discrimination\\\\ne)  no abusive practices\\\\n\\\\nforced or compulsory labor\\\\nfreedom of association and collective bargaining\\\\n\\\\no  strives to ensure internationally recognized labor rights and complies with all applicable local laws on\\\\n\\\\nlegal working hours\\\\n\\\\nworkers\\u2019 rights including:\\\\na)\\\\nb)  safe and healthy workplaces\\\\nc)  decent living wages\\\\nd)  gender equity\\\\n\\\\no  supports the livelihoods of rubber producers, particularly small farmers, and those communities in\\\\n\\\\nrubber producing areas\\\\n\\\\nDue Diligence\\\\n\\\\nTo demonstrate progress with our commitments and recognizing the influential role our company can play in\\\\ncontributing to sector-wide transformation, the BMW Group further implements or will implement following\\\\nmeasures in line with the BMW Group Supply Chain Due Diligence:\\\\n\\\\n-  Traceability: We will promote mapping and traceability of raw materials in our supply chain to a point\\\\n\\\\nat the appropriate jurisdictional level where compliance can be verified, aligned with the\\\\nrecommendations of the forthcoming GPSNR implementation guidance.\\\\n\\\\n-  Risk assessment: We will assess actual and potential environmental and social risks within our\\\\n\\\\nnatural rubber supply chains and will prioritize risk mitigation actions as defined in the forthcoming\\\\nGPSNR implementation guidance.\\\\n\\\\n-  Supplier engagement: We will work with our existing suppliers to identify, prevent and mitigate,\\\\n\\\\nenvironmental and social harms in natural rubber supply chains that violate this high-level\\\\ncommitment, and will evaluate the environmental and social performance of new suppliers/partners\\\\nprior to engagement. We require our suppliers to set up a due diligence process aligned with GPSNR\\\\npolicy framework for producing and procuring natural rubber and to implement necessary time-bound\\\\nmeasures as defined in the forthcoming GPSNR implementation guidelines. We offer guidance and\\\\ntraining to our suppliers to support them in understanding our expectations, relying on technical\\\\nsupport from GPSNR.\\\\n\\\\n-  Dispute resolution: We have a grievance mechanism in place (available by phone under +49 89\\\\n382-71230 or by e-mail under humanrights.sscm@bmwgroup.com.) to address any complaints\\\\nregarding our or our suppliers\\u2019 operations and to resolve disputes in a fair and timely manner. We also\\\\nexpect our suppliers to have their own grievance mechanism(s).\\\\n\\\\n-  Escalation: In cases our supply chains may not conform with our Supplier Sustainability Policy, we\\\\nwill develop time-bound implementation plans for moving towards conformance and remediation of\\\\npast or ongoing harms as defined in our escalation process.\\\\n\\\\n-  Reporting: We commit to reporting on our progress in implementing these commitments as specified\\\\n\\\\nin the forthcoming GPSNR reporting requirements.\\\\n\\\\n-  Multistakeholder Engagement: We commit to engaging in and contributing to multi-stakeholder,\\\\n\\\\nlandscape and supply chain initiatives like the GPSNR as well as further interventions that enable and\\\\nenhance sustainable natural rubber uptake in the global marketplace.\\\\n\\\\nBMW-Contact: sscm@bmw.de\\\\nStatus: August 2024\\\"}\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "final_dataset = video_dataset + audio_dataset + image_dataset + pdf_dataset\n",
    "print(f\"Total dataset rows: {len(final_dataset)}\")\n",
    "\n",
    "print(json.dumps(final_dataset, indent=2))  # print first 2 rows as a sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdbeaea",
   "metadata": {},
   "source": [
    "### Creating an Azure OpenAI Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cd71319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "# Use Entra ID authentication if AUTH_MODE is set to \"entra\"\n",
    "if AUTH_MODE == \"entra\":\n",
    "    # Get token provider for Azure OpenAI\n",
    "    from azure.identity import get_bearer_token_provider\n",
    "    \n",
    "    token_provider = get_bearer_token_provider(\n",
    "        credential, \n",
    "        \"https://cognitiveservices.azure.com/.default\"\n",
    "    )\n",
    "    \n",
    "    openai_client = AzureOpenAI(\n",
    "        azure_ad_token_provider=token_provider,\n",
    "        api_version=\"2024-02-15-preview\",\n",
    "        azure_endpoint=os.getenv(\"AZURE_AI_ENDPOINT\") or os.getenv(\"CONTENT_UNDERSTANDING_ENDPOINT\")\n",
    "    )\n",
    "else:\n",
    "    openai_client = AzureOpenAI(\n",
    "        api_key=os.getenv(\"AZURE_AI_API_KEY\"),\n",
    "        api_version=\"2024-02-15-preview\",\n",
    "        azure_endpoint=os.getenv(\"AZURE_AI_ENDPOINT\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40fbe8c",
   "metadata": {},
   "source": [
    "### Creating a Function for Generating Embeddings using Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "230ac057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "def generate_embeddings(text, embedding_model_name):\n",
    "    response = openai_client.embeddings.create(\n",
    "        input=text,\n",
    "        model=embedding_model_name\n",
    "    )\n",
    "\n",
    "    embeddings = response.model_dump()\n",
    "\n",
    "    return embeddings[\"data\"][0][\"embedding\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08683f41",
   "metadata": {},
   "source": [
    "### Finalising RAG Dataset with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dee661fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "for data in final_dataset:\n",
    "    embedding = generate_embeddings(data[\"content_text\"], os.getenv(\"EMBEDDING_MODEL_NAME\"))\n",
    "    data[\"content_embedding\"] = embedding\n",
    "    data[\"content_id\"] = uuid.uuid4().hex\n",
    "\n",
    "# printing the final dataset with embeddings in a file \n",
    "with open(\"data.json\", \"w\") as f:\n",
    "    f.write(json.dumps(final_dataset, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6695e7",
   "metadata": {},
   "source": [
    "### Creating our Azure AI Search Index in Azure Search Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e6a59541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code: 400\n",
      "Response Text: {\"error\":{\"code\":\"ResourceNameAlreadyInUse\",\"message\":\"Cannot create index 'multi-modal-rag-index' because it already exists.\",\"details\":[{\"code\":\"CannotCreateExistingIndex\",\"message\":\"Cannot create index 'multi-modal-rag-index' because it already exists.\"}]}}\n",
      "âŒ Error creating index: 400\n",
      "Response body: {'error': {'code': 'ResourceNameAlreadyInUse', 'message': \"Cannot create index 'multi-modal-rag-index' because it already exists.\", 'details': [{'code': 'CannotCreateExistingIndex', 'message': \"Cannot create index 'multi-modal-rag-index' because it already exists.\"}]}}\n"
     ]
    }
   ],
   "source": [
    "search_service_endpoint = os.getenv(\"SEARCH_SERVICE_ENDPOINT\").strip().rstrip('/')\n",
    "search_service_api_key = os.getenv(\"SEARCH_SERVICE_API_KEY\")\n",
    "\n",
    "index_creation_url = search_service_endpoint + \"/indexes?api-version=2025-08-01-preview\"\n",
    "\n",
    "# Set up headers based on auth mode\n",
    "if AUTH_MODE == \"entra\":\n",
    "    # Get token for Azure Search\n",
    "    search_token = credential.get_token(\"https://search.azure.com/.default\").token\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {search_token}\"\n",
    "    }\n",
    "else:\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": search_service_api_key\n",
    "    }\n",
    "\n",
    "# reading the index schema from index.json file\n",
    "with open(\"index.json\", \"r\") as f:\n",
    "    index_schema = json.load(f)\n",
    "\n",
    "body = index_schema \n",
    "\n",
    "response = requests.post(index_creation_url, headers=headers, json=body)\n",
    "\n",
    "print(f\"Status Code: {response.status_code}\")\n",
    "print(f\"Response Text: {response.text[:500] if response.text else '(empty)'}\")\n",
    "\n",
    "if response.status_code == 201:\n",
    "    print(\"âœ… Index created successfully.\")\n",
    "elif response.status_code == 200:\n",
    "    print(\"âœ… Index already exists or was updated.\")\n",
    "elif response.status_code == 204:\n",
    "    print(\"âœ… Success (no content returned).\")\n",
    "else:\n",
    "    print(f\"âŒ Error creating index: {response.status_code}\")\n",
    "    if response.text:\n",
    "        try:\n",
    "            print(\"Response body:\", response.json())\n",
    "        except:\n",
    "            print(\"Response body (raw):\", response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe61463",
   "metadata": {},
   "source": [
    "### Preparing the RAG Data for Bulk Upload to Azure AI Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d118faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "for object in dataset:\n",
    "    object[\"@search.action\"] = \"upload\" # append the action to each object\n",
    "\n",
    "with open(\"data_with_actions.json\", \"w\") as f:\n",
    "    f.write(json.dumps(dataset, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5d4cb4",
   "metadata": {},
   "source": [
    "### Pushing RAG formatted data to Azure AI Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de81611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during bulk upload: 403 \n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\requests\\models.py:974\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03mcontaining a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\json\\decoder.py:355\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during bulk upload:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m.\u001b[39mtext)\n\u001b[1;32m---> 32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponse body:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\requests\\models.py:978\u001b[0m, in \u001b[0;36mResponse.json\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson\u001b[38;5;241m.\u001b[39mloads(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[1;32m--> 978\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e\u001b[38;5;241m.\u001b[39mmsg, e\u001b[38;5;241m.\u001b[39mdoc, e\u001b[38;5;241m.\u001b[39mpos)\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "with open(\"data_with_actions.json\", \"r\") as f:\n",
    "    rag_dataset = json.load(f)\n",
    "\n",
    "\n",
    "bulk_upload_url = search_service_endpoint + \"/indexes/multi-modal-rag-index/docs/index?api-version=2025-08-01-preview\"\n",
    "\n",
    "# Set up headers based on auth mode\n",
    "if AUTH_MODE == \"entra\":\n",
    "    # Get fresh token for Azure Search\n",
    "    search_token = credential.get_token(\"https://search.azure.com/.default\").token\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {search_token}\"\n",
    "    }\n",
    "else:\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": search_service_api_key\n",
    "    }\n",
    "\n",
    "body = {\n",
    "    \"value\": [*rag_dataset]\n",
    "}\n",
    "\n",
    "response = requests.post(bulk_upload_url, headers=headers, json=body)\n",
    "\n",
    "print(f\"Status Code: {response.status_code}\")\n",
    "print(f\"Response Text: {response.text[:1000] if response.text else '(empty)'}\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(\"âœ… Bulk upload successful.\")\n",
    "elif response.status_code == 207:\n",
    "    print(\"âš ï¸ Partial success - some documents may have failed.\")\n",
    "else:\n",
    "    print(f\"âŒ Error during bulk upload: {response.status_code}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
